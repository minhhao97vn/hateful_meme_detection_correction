{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd34161-1353-4bb2-93dc-c972ec809150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import re\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663d081-f0de-4a9e-8e2b-1beb9db5fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key='<your-api-key>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ef0a9-f80c-405d-aafd-032617ac75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b08a4c-ee81-488e-a240-7c0e5c5a4927",
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_settings = [{\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "                   {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"}, \n",
    "                   {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "                   {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b164c54-4d74-4a39-be57-0b2f77e4dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('models/gemini-1.5-flash-latest', safety_settings = safety_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1454ca94-52c7-4a10-b632-1478c7b6eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import json\n",
    "\n",
    "with open(\"/home/haovan/hateful_memes/test_unseen\" + '.jsonl', 'r') as file:\n",
    "    test_data_meta = list(file)\n",
    "\n",
    "with open(\"/home/haovan/hateful_memes/train\" + '.jsonl', 'r') as file:\n",
    "    train_data_meta = list(file)\n",
    "\n",
    "img_dir = '/home/haovan/hateful_memes/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cb324f-57f9-49cb-819f-40588b190a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {}\n",
    "test_data = {}\n",
    "for idx, json_str in enumerate(train_data_meta):\n",
    "    train_sample = json.loads(json_str)\n",
    "    train_data[train_sample['id']] = train_sample\n",
    "for idx, json_str in enumerate(test_data_meta):\n",
    "    test_sample = json.loads(json_str)\n",
    "    test_data[test_sample['id']] = test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6a59ab-e31e-48c6-b08c-8c12018cea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = torch.load(\"/home/haovan/hateful_memes/test_unseen_meme_embeddings.pt\")\n",
    "train_embeddings = torch.load(\"/home/haovan/hateful_memes/train_meme_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4e8e94-f3e6-416f-b1d8-1b21c7bf4ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_train_scores = {}\n",
    "for test_id in test_embeddings.keys():\n",
    "    scores = []\n",
    "    test_embedding = test_embeddings[test_id]\n",
    "    for train_id in train_embeddings.keys():\n",
    "        scores.append((train_id, F.cosine_similarity(test_embedding, train_embeddings[train_id]).cpu()))\n",
    "    print(test_id)\n",
    "    test_train_scores[test_id] = scores\n",
    "torch.save(test_train_scores, \"unseen_test_train_scores.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fbd142-8616-43f3-b71d-1f198815592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_scores = torch.load(\"unseen_test_train_scores.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0224e446-426a-44e6-a222-b25842550616",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_processed_idx = -1\n",
    "unprocessed_ids = []\n",
    "responses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea2645a-e654-4957-b558-a7c969c33a15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt_template = lambda text, label: f\"\"\"Classification: {label}\\n\"\"\"\n",
    "\n",
    "train_ids = list(train_embeddings.keys())\n",
    "\n",
    "num_shots = 4\n",
    "\n",
    "inst = f\"\"\"Hatefulness definition: Hate speech is strictly defined as a direct or indirect attack on people based on characteristics, including ethnicity, race, nationality, immigration status, religion, caste, sex, gender identity, sexual orientation, disability, or disease. The definition of attack includes violent or dehumanizing speech, statements of inferiority, and calls for exclusion or segregation. Mocking hate crime is also considered hate speech.\n",
    "Classification criteria: The meme is hateful if one or both of the visual and textual meanings are hateful. If both visual and textual meanings are non-hateful but together they lead to implicitly hateful meaning, the meme is also hateful. Otherwise, the meme is non-hateful.\n",
    "You consider visual and textual meanings to classify the meme as hateful or non-hateful based on the hatefulness definition and classification criteria.\\n\\n\"\"\"\n",
    "\n",
    "for idx, test_id in enumerate(test_data.keys()):\n",
    "    if idx <= last_processed_idx:\n",
    "        continue\n",
    "    test_sample = test_data[test_id]\n",
    "        \n",
    "    sorted_scores = sorted(test_train_scores[test_id], key = lambda x: x[1], reverse=True)\n",
    "    icl_prompt = [\"Here are some demonstrations on classifying memes:\\n\"]\n",
    "    train_samples = [None for i in range(num_shots)]\n",
    "    ct_non = 0\n",
    "    ct_hate = 0\n",
    "    for (train_id, _) in sorted_scores:\n",
    "        train_label = train_data[train_id]['label']\n",
    "        if train_label == 0 and ct_non < int(num_shots/2):\n",
    "            train_samples[ct_non*2] = train_data[train_id]\n",
    "            ct_non += 1\n",
    "        elif train_label == 1 and ct_hate < int(num_shots/2):\n",
    "            train_samples[ct_hate*2+1] = train_data[train_id]\n",
    "            ct_hate += 1\n",
    "        if ct_hate == int(num_shots/2) and ct_non == int(num_shots/2):\n",
    "            break\n",
    "    for train_sample in train_samples:\n",
    "        img = PIL.Image.open(img_dir+train_sample['img'])\n",
    "        label = \"hateful\" if train_sample['label'] == 1 else \"non-hateful\"\n",
    "        prompt = prompt_template(train_sample['text'], label)\n",
    "        icl_prompt += [img]\n",
    "        icl_prompt += [prompt]\n",
    "\n",
    "    print(f\"Processing image: {test_id}, label: {test_sample['label']}\")\n",
    "    prompt = f\"\"\"For this test image, please give the classification and probability of the meme being hateful (from 0 to 1) in the following format:\n",
    "Classification: \n",
    "Probability:\"\"\"\n",
    "    test_img = PIL.Image.open(img_dir+test_sample['img'])\n",
    "    final_prompt = [inst]+icl_prompt+[test_img, prompt]\n",
    "    response = model.generate_content(final_prompt)\n",
    "    try:\n",
    "        answer = response.text\n",
    "    except Exception as e:\n",
    "        unprocessed_ids.append(test_sample['img'])\n",
    "        answer = \"MODEL ERROR\"\n",
    "    print(answer)\n",
    "    responses.append(answer)\n",
    "    last_processed_idx = idx\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8acd2e6-d01e-4563-a03f-3f1a31a7cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.prompt_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad512026-da42-4e1a-8f5c-cdd55f9784b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unprocessed_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cdf905-df36-4423-809f-30150a299504",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'results_gemini_unseen_test_prompt_icl_{num_shots}_shots.pkl'\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump({\"responses\": responses, \"unprocessed_ids\": unprocessed_ids}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03939acb-c035-4d47-9085-21f8b4fceecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_name, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    responses = data['responses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c2bbd7-0019-4691-a813-2829793fdc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e826fd3f-8aa9-4e79-a17d-ff1125a2392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = []\n",
    "predictions = []\n",
    "prob_list = []\n",
    "\n",
    "for idx, json_str in enumerate(test_data_meta):\n",
    "    test_sample = json.loads(json_str)\n",
    "    actual = test_sample['label']\n",
    "    actuals.append(actual)\n",
    "\n",
    "    lower_response = responses[idx].lower()\n",
    "    all_found = re.findall(\"probability.*?[0-9]+\\.?[0-9]*\", lower_response)\n",
    "\n",
    "    if len(all_found) == 0:\n",
    "        hateful_keywords = [\"classification: hateful\"]\n",
    "\n",
    "        is_hateful = False\n",
    "        for kw in hateful_keywords:\n",
    "            if kw in lower_response:\n",
    "                is_hateful = True\n",
    "                break\n",
    "        if is_hateful:\n",
    "            predicted_class = 1\n",
    "            prob = 1.0\n",
    "        elif \"classification: non-hateful\" in lower_response:\n",
    "            predicted_class = 0\n",
    "            prob = 0.0\n",
    "        else:\n",
    "            pedicted_class = 1\n",
    "            prob = 1.0\n",
    "        print(f'Idx: {idx} - Res: {lower_response}')\n",
    "        print()\n",
    "    else:\n",
    "        num_str = re.sub('[^0-9\\.]', '', (all_found[0].split(\" \"))[-1])\n",
    "        if num_str[-1] == '.':\n",
    "            num_str = num_str[:-1]\n",
    "        if float(num_str) >= 0.5:\n",
    "            predicted_class = 1\n",
    "        else:\n",
    "            predicted_class = 0\n",
    "        prob = float(num_str)\n",
    "    prob_list.append(prob)\n",
    "    predictions.append(predicted_class)\n",
    "\n",
    "acc = np.mean(np.array(actuals) == np.array(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e688da-56a3-4764-a600-f2a6b5786955",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"AUROC: {roc_auc_score(actuals, prob_list)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
